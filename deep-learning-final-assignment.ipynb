{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ０～９までの数字の画像からどの数字なのかを推論する畳み込みニューラルネットワーク（CNN）モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\deep-learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    image = ds['train'][i]['image']\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f\"Label: {ds['train'][i]['label']}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像サイズ (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(f\"画像サイズ {image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 畳み込みニューラルネットワークのモデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数とオプティマイザーを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(type='torch', columns=['image', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 7.407\n",
      "[1, 200] loss: 7.460\n",
      "[1, 300] loss: 7.411\n",
      "[1, 400] loss: 7.392\n",
      "[1, 500] loss: 7.530\n",
      "[1, 600] loss: 7.483\n",
      "[1, 700] loss: 7.385\n",
      "[1, 800] loss: 7.493\n",
      "[1, 900] loss: 7.514\n",
      "[2, 100] loss: 7.430\n",
      "[2, 200] loss: 7.460\n",
      "[2, 300] loss: 7.520\n",
      "[2, 400] loss: 7.430\n",
      "[2, 500] loss: 7.435\n",
      "[2, 600] loss: 7.486\n",
      "[2, 700] loss: 7.445\n",
      "[2, 800] loss: 7.526\n",
      "[2, 900] loss: 7.425\n",
      "[3, 100] loss: 7.461\n",
      "[3, 200] loss: 7.461\n",
      "[3, 300] loss: 7.426\n",
      "[3, 400] loss: 7.454\n",
      "[3, 500] loss: 7.448\n",
      "[3, 600] loss: 7.437\n",
      "[3, 700] loss: 7.561\n",
      "[3, 800] loss: 7.459\n",
      "[3, 900] loss: 7.397\n",
      "[4, 100] loss: 7.364\n",
      "[4, 200] loss: 7.501\n",
      "[4, 300] loss: 7.475\n",
      "[4, 400] loss: 7.435\n",
      "[4, 500] loss: 7.501\n",
      "[4, 600] loss: 7.391\n",
      "[4, 700] loss: 7.374\n",
      "[4, 800] loss: 7.535\n",
      "[4, 900] loss: 7.488\n",
      "[5, 100] loss: 7.426\n",
      "[5, 200] loss: 7.472\n",
      "[5, 300] loss: 7.374\n",
      "[5, 400] loss: 7.473\n",
      "[5, 500] loss: 7.442\n",
      "[5, 600] loss: 7.395\n",
      "[5, 700] loss: 7.483\n",
      "[5, 800] loss: 7.498\n",
      "[5, 900] loss: 7.446\n",
      "[6, 100] loss: 7.442\n",
      "[6, 200] loss: 7.439\n",
      "[6, 300] loss: 7.458\n",
      "[6, 400] loss: 7.572\n",
      "[6, 500] loss: 7.392\n",
      "[6, 600] loss: 7.469\n",
      "[6, 700] loss: 7.458\n",
      "[6, 800] loss: 7.422\n",
      "[6, 900] loss: 7.446\n",
      "[7, 100] loss: 7.478\n",
      "[7, 200] loss: 7.376\n",
      "[7, 300] loss: 7.492\n",
      "[7, 400] loss: 7.477\n",
      "[7, 500] loss: 7.400\n",
      "[7, 600] loss: 7.483\n",
      "[7, 700] loss: 7.426\n",
      "[7, 800] loss: 7.382\n",
      "[7, 900] loss: 7.528\n",
      "[8, 100] loss: 7.502\n",
      "[8, 200] loss: 7.512\n",
      "[8, 300] loss: 7.502\n",
      "[8, 400] loss: 7.469\n",
      "[8, 500] loss: 7.455\n",
      "[8, 600] loss: 7.438\n",
      "[8, 700] loss: 7.325\n",
      "[8, 800] loss: 7.436\n",
      "[8, 900] loss: 7.438\n",
      "[9, 100] loss: 7.510\n",
      "[9, 200] loss: 7.370\n",
      "[9, 300] loss: 7.476\n",
      "[9, 400] loss: 7.386\n",
      "[9, 500] loss: 7.443\n",
      "[9, 600] loss: 7.490\n",
      "[9, 700] loss: 7.445\n",
      "[9, 800] loss: 7.480\n",
      "[9, 900] loss: 7.502\n",
      "[10, 100] loss: 7.545\n",
      "[10, 200] loss: 7.467\n",
      "[10, 300] loss: 7.299\n",
      "[10, 400] loss: 7.519\n",
      "[10, 500] loss: 7.498\n",
      "[10, 600] loss: 7.409\n",
      "[10, 700] loss: 7.504\n",
      "[10, 800] loss: 7.448\n",
      "[10, 900] loss: 7.351\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# データローダーの作成\n",
    "train_loader = torch.utils.data.DataLoader(ds['train'], batch_size=64, shuffle=True)\n",
    "\n",
    "# モデルの訓練\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data['image'], data['label']\n",
    "        inputs = inputs.squeeze(2).float()  # チャンネル次元を追加し、floatに変換\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # 100ミニバッチごとに平均損失を出力\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(ds['train'], batch_size=4, shuffle=False)\n",
    "\n",
    "preds_list = []\n",
    "actuals_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data['image'], data['label']\n",
    "        images = images.squeeze(2).float()\n",
    "        outpts = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        preds_list.extend(preds)\n",
    "        actuals_list.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ヒートマップでビジュアライズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [60000, 480000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactuals_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      7\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\deep-learning\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mf:\\deep-learning\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:328\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    234\u001b[0m     {\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m ):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mf:\\deep-learning\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\deep-learning\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:460\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    458\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    463\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [60000, 480000]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(actuals_list, preds_list)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"predict\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
