{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学生時代は物理学を研究。力学系、複雑系とよばれる分野。\n",
    "    - 5次方程式の解の公式について (Évariste Galois)\n",
    "    - Strange Attractor：https://www.youtube.com/watch?v=oqDQwEvHGfE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工知能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking Machine\n",
    "- A. Turing, Computing Machinery and Intelligence, Mind, LIX (236): 433–460 (1950).\n",
    "    - 最初の人工知能の論文と言われることもある。\n",
    "    - Turing Test\n",
    "    - [The Imitation Game](https://imitationgame.gaga.ne.jp/)\n",
    "\n",
    "Neuronの数学的モデル (1943年)\n",
    "- W. S. McCulloch and W. Pitts, A logical calculus of the ideas immanent in nervous activity, The Bulletin of Mathematical Biophysics. 5 (4): 115–133 (1943).\n",
    "- ニューラルネットの起源\n",
    "- 神経生理学者で外科医のMcCullochと、数学者のPitts。\n",
    "- 閾値, 階段関数.\n",
    "\n",
    "Dartmouth Conference (1956年)\n",
    "- 初めて「artificial intelligence」という言葉が使われた。\n",
    "- https://web.archive.org/web/20070826230310/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html\n",
    "\n",
    "Perceptron (1958年)\n",
    "- F. Rosenblatt, The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Psychological Review 65 (6): 386-408 (1958).\n",
    "    - 「Mark I Perceptron」という機械で実装.\n",
    "- 同じ1958年に提案されたLogistic回帰と同じモデル(1層のニューラルネット)。\n",
    "- McCullochとPittsのモデルと違って、重みを更新する。\n",
    "\n",
    "確率的勾配降下法 (1967年)\n",
    "- S. Amari, Theories of adaptive pattern classifiers,\n",
    "IEEE Trans., EC-16, pp. 299-307（1967）\n",
    "- 多層Perceptronに初めて確率的勾配降下法を導入。\n",
    "\n",
    "Backpropagation (1986年)\n",
    "- D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors, nature 323.6088: 533-536 (1986).\n",
    "\n",
    "AlexNet\n",
    "- 2012年の画像コンペで優勝。\n",
    "- AlexNetにつながる2つの研究：\n",
    "    - 1980年、福島邦彦のNeocognitron (畳み込みニューラルネット).\n",
    "    - 1989年、Yann LeCun などのLeNet (畳み込みニューラルネット + Backpropagation).\n",
    "\n",
    "AlphaGo\n",
    "- 2016年、プロ棋士の イ セドル に勝利。\n",
    "- 映画：https://www.alphagomovie.com\n",
    "\n",
    "Transformer (2017年)\n",
    "- [\"Attention is All you Need\"](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) で提案された翻訳モデル。\n",
    "\n",
    "Turing賞 (2018年)\n",
    "- Benngio / Hinton / LeCun\n",
    "\n",
    "ChatGPT\n",
    "- 2022年、GPT-3.5 を公開。\n",
    "\n",
    "ノーベル賞 2024年\n",
    "- ニューラルネットの研究で、HopfieldとHintonに物理学賞が贈られた。\n",
    "- 化学賞も人工知能研究によるもの。DeepMind社のAlphaFold2など。\n",
    "- 人工知能の研究で、ノーベル賞を獲ったのは初めて。\n",
    "\n",
    "---\n",
    "\n",
    "Deep Learning を学ぶために必要な数学\n",
    "- 微分と積分\n",
    "- 線形代数\n",
    "- 確率論 + 統計学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **.py と **.ipynb の違い\n",
    "- 関数\n",
    "    - return\n",
    "    - Argument 引数\n",
    "    - Default Argument\n",
    "- Class\n",
    "    - \\__init__\n",
    "    - クラス変数とインスタンス変数\n",
    "    - メソッド\n",
    "- ライブラリのinstall/importをする方法\n",
    "- この講義で使うライブラリ\n",
    "    - Pandas\n",
    "    - Matplotlib / Seaborn\n",
    "    - scikit-learn\n",
    "    - PyTorch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
